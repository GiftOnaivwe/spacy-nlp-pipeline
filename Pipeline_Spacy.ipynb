{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b59189e8",
   "metadata": {},
   "source": [
    "# spaCy NLP Pipeline\n",
    "\n",
    "This notebook demonstrates a clean, production‑minded mini NLP pipeline using **spaCy**.\n",
    "\n",
    "**What it does**\n",
    "- Loads a spaCy model with graceful fallback/download if missing\n",
    "- Tokenization with POS & lemma filtering\n",
    "- Stopword/punctuation handling\n",
    "- Noun chunks extraction\n",
    "- Named Entity Recognition (NER)\n",
    "- Dependency parse summary (head/dependency)\n",
    "- Saves tidy outputs to `outputs/` as CSV\n",
    "- Small self‑check to validate the pipeline runs end‑to‑end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a024915",
   "metadata": {},
   "source": [
    "## Setup & Install\n",
    "This cell ensures the required spaCy model is available. It tries to load `en_core_web_sm` and downloads it if missing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd290504",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running on a fresh environment, uncomment the next line to install spaCy\n",
    "# pip install spacy==3.7.2\n",
    "\n",
    "import sys, subprocess, importlib\n",
    "import spacy\n",
    "from typing import List, Dict, Tuple\n",
    "import json, pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "from pathlib import Path\n",
    "\n",
    "def ensure_spacy_model(model_name: str = \"en_core_web_sm\"):\n",
    "    try:\n",
    "        import spacy\n",
    "        spacy.load(model_name)\n",
    "        return model_name\n",
    "    except Exception:\n",
    "        # Try to download the model\n",
    "        print(f\"Downloading spaCy model: {model_name} ...\")\n",
    "        subprocess.run([sys.executable, \"-m\", \"spacy\", \"download\", model_name], check=False)\n",
    "        import spacy as _sp\n",
    "        _sp.load(model_name)  # will raise if still missing\n",
    "        return model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4de08528-b169-4a18-9eb3-13d4f393723f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy version: 3.7.2 | model: en_core_web_sm\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = ensure_spacy_model(\"en_core_web_sm\")\n",
    "\n",
    "nlp = spacy.load(MODEL_NAME)\n",
    "print(\"spaCy version:\", spacy.__version__, \"| model:\", MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dc37da",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Tweak the lists below to customize which parts of speech to keep and whether to lemmatize.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d681ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'keep_pos': ('NOUN', 'PROPN', 'VERB', 'ADJ', 'ADV', 'NUM'),\n",
       " 'drop_deps': ('punct',),\n",
       " 'lowercase': True,\n",
       " 'use_lemma': True}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "@dataclass\n",
    "class PipelineConfig:\n",
    "    keep_pos: tuple = (\"NOUN\",\"PROPN\",\"VERB\",\"ADJ\",\"ADV\",\"NUM\")\n",
    "    drop_deps: tuple = (\"punct\",)\n",
    "    lowercase: bool = True\n",
    "    use_lemma: bool = True\n",
    "\n",
    "CONFIG = PipelineConfig()\n",
    "asdict(CONFIG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc4d9b8",
   "metadata": {},
   "source": [
    "## Pipeline Functions\n",
    "Modular helpers that:\n",
    "- Clean and normalize tokens\n",
    "- Produce tidy DataFrames for tokens, entities, and dependencies\n",
    "- Save results to `outputs/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1dfaaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = Path(\"outputs\")\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def analyze_text(text: str, cfg: PipelineConfig = CONFIG) -> Dict[str, pd.DataFrame]:\n",
    "    doc = nlp(text)\n",
    "    rows = []\n",
    "    for tok in doc:\n",
    "        if tok.is_space or tok.is_punct or tok.pos_ == \"X\":\n",
    "            continue\n",
    "        if tok.pos_ not in cfg.keep_pos:\n",
    "            continue\n",
    "        form = tok.lemma_ if cfg.use_lemma else tok.text\n",
    "        if cfg.lowercase:\n",
    "            form = form.lower()\n",
    "        rows.append({\n",
    "            \"text\": tok.text,\n",
    "            \"norm\": form,\n",
    "            \"pos\": tok.pos_,\n",
    "            \"lemma\": tok.lemma_,\n",
    "            \"is_stop\": tok.is_stop,\n",
    "            \"dep\": tok.dep_,\n",
    "            \"head\": tok.head.text,\n",
    "            \"i\": tok.i,\n",
    "        })\n",
    "    tokens_df = pd.DataFrame(rows)\n",
    "\n",
    "    ents_df = pd.DataFrame([{\"text\": ent.text, \"label\": ent.label_, \"start\": ent.start_char, \"end\": ent.end_char}\n",
    "                            for ent in doc.ents])\n",
    "\n",
    "    deps_df = pd.DataFrame([{\n",
    "        \"text\": t.text, \"pos\": t.pos_, \"dep\": t.dep_, \"head\": t.head.text, \"head_pos\": t.head.pos_\n",
    "    } for t in doc])\n",
    "\n",
    "    noun_chunks_df = pd.DataFrame([{\"chunk\": chunk.text, \"root\": chunk.root.text, \"root_dep\": chunk.root.dep_}\n",
    "                                   for chunk in doc.noun_chunks])\n",
    "\n",
    "    return {\n",
    "        \"tokens\": tokens_df,\n",
    "        \"entities\": ents_df,\n",
    "        \"dependencies\": deps_df,\n",
    "        \"noun_chunks\": noun_chunks_df\n",
    "    }\n",
    "\n",
    "def save_outputs(dfs: Dict[str, pd.DataFrame], prefix: str = \"demo\"):\n",
    "    for name, df in dfs.items():\n",
    "        out = OUT_DIR / f\"{prefix}_{name}.csv\"\n",
    "        df.to_csv(out, index=False)\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58474328",
   "metadata": {},
   "source": [
    "## Demo Input\n",
    "Change the sample text below or load from `sample_text.txt` (creat txt file in your notebook dir).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c515f67a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This AI program at CCBST explores Natural Language Processing (NLP) with spaCy.\\nWe analyze text, extract entities like Apple and Toronto General Hospital, and inspect dependencies.\\nIn 2025, our team improved model accuracy by 12% while reducing inference latency.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "SAMPLE_TEXT = 'This AI program at CCBST explores Natural Language Processing (NLP) with spaCy.\\nWe analyze text, extract entities like Apple and Toronto General Hospital, and inspect dependencies.\\nIn 2025, our team improved model accuracy by 12% while reducing inference latency.'\n",
    "SAMPLE_TEXT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81900cc",
   "metadata": {},
   "source": [
    "## Run the Pipeline\n",
    "Produces tidy dataframes and saves them under `outputs/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a4f4241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens':          text        norm    pos       lemma  is_stop       dep      head   i\n",
       " 0          AI          ai  PROPN          AI    False  compound   program   1\n",
       " 1     program     program   NOUN     program    False     nsubj  explores   2\n",
       " 2       CCBST       ccbst  PROPN       CCBST    False      pobj        at   4\n",
       " 3    explores     explore   VERB     explore    False      ROOT  explores   5\n",
       " 4     Natural     natural  PROPN     Natural    False  compound  Language   6\n",
       " 5    Language    language  PROPN    Language    False      nmod       NLP   7\n",
       " 6  Processing  processing  PROPN  Processing    False      nmod       NLP   8\n",
       " 7         NLP         nlp  PROPN         NLP    False      dobj  explores  10\n",
       " 8       spaCy       spacy  PROPN       spaCy    False      pobj      with  13\n",
       " 9     analyze     analyze   VERB     analyze    False      ROOT   analyze  17,\n",
       " 'entities':                           text    label  start  end\n",
       " 0                           AI      ORG      5    7\n",
       " 1  Natural Language Processing      ORG     34   61\n",
       " 2                          NLP      ORG     63   66\n",
       " 3                        Apple      ORG    119  124\n",
       " 4     Toronto General Hospital      ORG    129  153\n",
       " 5                         2025     DATE    184  188\n",
       " 6                          12%  PERCENT    226  229,\n",
       " 'dependencies':          text    pos       dep      head head_pos\n",
       " 0        This    DET       det   program     NOUN\n",
       " 1          AI  PROPN  compound   program     NOUN\n",
       " 2     program   NOUN     nsubj  explores     VERB\n",
       " 3          at    ADP      prep   program     NOUN\n",
       " 4       CCBST  PROPN      pobj        at      ADP\n",
       " 5    explores   VERB      ROOT  explores     VERB\n",
       " 6     Natural  PROPN  compound  Language    PROPN\n",
       " 7    Language  PROPN      nmod       NLP    PROPN\n",
       " 8  Processing  PROPN      nmod       NLP    PROPN\n",
       " 9           (  PUNCT     punct       NLP    PROPN,\n",
       " 'noun_chunks':                               chunk          root root_dep\n",
       " 0                   This AI program       program    nsubj\n",
       " 1                             CCBST         CCBST     pobj\n",
       " 2  Natural Language Processing (NLP           NLP     dobj\n",
       " 3                             spaCy         spaCy     pobj\n",
       " 4                                We            We    nsubj\n",
       " 5                              text          text     dobj\n",
       " 6                          entities      entities     dobj\n",
       " 7                             Apple         Apple     pobj\n",
       " 8          Toronto General Hospital      Hospital     conj\n",
       " 9                      dependencies  dependencies     dobj}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dfs = analyze_text(SAMPLE_TEXT, CONFIG)\n",
    "# Display a compact preview\n",
    "preview = {k: v.head(10) for k, v in dfs.items()}\n",
    "preview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24022b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['outputs\\\\sample_dependencies.csv',\n",
       " 'outputs\\\\sample_entities.csv',\n",
       " 'outputs\\\\sample_noun_chunks.csv',\n",
       " 'outputs\\\\sample_tokens.csv']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "save_outputs(dfs, prefix=\"sample\")\n",
    "sorted([str(p) for p in OUT_DIR.glob(\"sample_*.csv\")])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d429647",
   "metadata": {},
   "source": [
    "## Quick Self‑Check\n",
    "A tiny smoke test to prove the notebook works end‑to‑end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7576b943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Basic self-checks passed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "assert \"tokens\" in dfs and not dfs[\"tokens\"].empty, \"Tokens DF is empty\"\n",
    "# dependencies includes all tokens (even those filtered out earlier), so should be >= tokens\n",
    "assert len(dfs[\"dependencies\"]) >= len(dfs[\"tokens\"]), \"Dependencies DF looks wrong\"\n",
    "print(\"✅ Basic self-checks passed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
